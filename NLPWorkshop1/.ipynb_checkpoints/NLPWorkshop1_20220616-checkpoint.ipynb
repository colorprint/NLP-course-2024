{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca0fb38",
   "metadata": {},
   "source": [
    "# NLP Workshop 1\n",
    "# NLP Basics with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5543f13",
   "metadata": {},
   "source": [
    "## 10.1. What is NLTK?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42026db7",
   "metadata": {},
   "source": [
    "In order to deal with the human language data that used in statistical natural language processing (NLP) application, Natural Language Toolkit (NLTK) is designed for building Python programs.\n",
    "\n",
    "Offical site of NLTK -- https://www.nltk.org/. \n",
    "\n",
    "It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania, bundled with their classical book Natural Language Processing with Python published by O'Reilly Media Inc in 2009.\n",
    "\n",
    "There are 32 universities in the US and 25 countries using NLTK in their courses.\n",
    "\n",
    "Although the textbook haven't update from 2009 and all the implementations are done under Python 2.X, still this book can be considered as the \"Bible\" and \"Starting-point\" for anyone who would like to learn applied NLP and implement NLP applications using Python. \n",
    "\n",
    "The latest version of NLTK now (as of 22 Jan 2022) is version 3.6.5.\n",
    "\n",
    "NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet - WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept and also one of the most important the fundamental lexical database in NLP world, developed by Pricenton University from 1980's https://wordnet.princeton.edu/.\n",
    "\n",
    "Other lexical databases and corpora such as the Penn Treebank Corpus, Open Multilingual Wordnet, Problem Report Corpus, and Lin’s Dependency Thesaurus.\n",
    "\n",
    "In fact,the most important feature of NLTK is that it contains the basic statistical-based text processing libraries for FIVE fundamental NLP enabling technology together with basic semantic reasoning tool, which include: \n",
    "- tokenization\n",
    "- parsing\n",
    "- classification\n",
    "- stemming\n",
    "- tagging\n",
    "- basic semantic reasoning\n",
    "\n",
    "It also includes graphical demonstrations and sample data sets as well as accompanied by a cook book and a book which explains the principles behind the underlying language processing tasks that NLTK supports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb63ed2",
   "metadata": {},
   "source": [
    "## 10.2. A Taste of NLTK on Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de225c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e265f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"On every weekend, early in the morning. I drive my car to the car center for car washing. Like clock-work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6753e3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On every weekend, early in the morning. I drive my car to the car center for car washing. Like clock-work.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d6fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7a72e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On',\n",
       " 'every',\n",
       " 'weekend',\n",
       " ',',\n",
       " 'early',\n",
       " 'in',\n",
       " 'the',\n",
       " 'morning',\n",
       " '.',\n",
       " 'I',\n",
       " 'drive',\n",
       " 'my',\n",
       " 'car',\n",
       " 'to',\n",
       " 'the',\n",
       " 'car',\n",
       " 'center',\n",
       " 'for',\n",
       " 'car',\n",
       " 'washing',\n",
       " '.',\n",
       " 'Like',\n",
       " 'clock-work',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ab004",
   "metadata": {},
   "source": [
    "## 10.3. How to Install NLTK?\n",
    "#### Step 1 Install Python 3.X\n",
    "#### Step 2 Install NLTK\n",
    "#### 2.1 Start CMD or other Command Line Tool\n",
    "#### 2.2 Type 'pip install nltk'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99dec802",
   "metadata": {},
   "source": [
    "<img src=\"./Fig10.1.png\" width = \"\" height = \"\" alt=\"NLTK installation\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef1e80",
   "metadata": {},
   "source": [
    "#### Step 3 Install NLTK Data\n",
    "#### Once you finished install NLTK into Python, you can download the NLTK Data\n",
    "#### 3.1 Run Python\n",
    "#### 3.2 Type the following to activate the NLTK downloader.\n",
    "- import nltk\n",
    "- nltk.download()\n",
    "\n",
    "Note: nltk.downloader() will automatically invoke the NLTK downloader, a separate window-based downloading module for user to download FOUR different types of NLP DATA into their Python machines. They include: Collection libraries, Corpora, Modules and other NLP packages. \n",
    "\n",
    "<img src=\"./Fig10.2.png\" width = \"\" height = \"\" alt=\"NLTK installation\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f246c7",
   "metadata": {},
   "source": [
    "It will automatically install the related Corpora library, NLTK models and Packages like this:\n",
    "\n",
    "##### \n",
    "##### Corpora library\n",
    "<img src=\"./Fig10.3.png\" width = \"\" height = \"\" alt=\"NLTK installation\" align=left />\n",
    "\n",
    "##### \n",
    "##### NLTK models\n",
    "<img src=\"./Fig10.4.png\" width = \"\" height = \"\" alt=\"NLTK installation\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86094b3a",
   "metadata": {},
   "source": [
    "## 10.4. Why using Python for NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10699041",
   "metadata": {},
   "source": [
    "Before the popularity of Python in AI and NLP, C, C++ and later on Java dominate the world of software development. \n",
    "However, started from early 2000, Python and their associate toolkit and packages start to dominate the world of software development, especially in the areas of Data Science, AI and NLP. \n",
    "\n",
    "Several reasons to drive for the changes:\n",
    "1. Python is a generalist language, means that it does not specialize in one area. \n",
    "2. Other commonly used language such as Java and JavaScript, on the other hand, is specifically designed for use on the web, thet are most suitable for developing web applications and websites. \n",
    "3. Python, on the other hand, is a generalist language, which means it can be used for a wide variety of purposes, including:\n",
    "- Data Science Analysis and Applications\n",
    "- Developing web apps\n",
    "- Creating software (both Web or Non-web based)\n",
    "- AI modeling and applications (e.g. building Deep Networks)\n",
    "- Natural language processing \n",
    "4. Easy to learn and use. As compared with C and C++, Python is much easier to learn. Especially useful for non-computer science students and scientists. \n",
    "5. In term of NLP, Python's list and list-processing data-type provide an excellent environment for the NLP modeling. \n",
    "\n",
    "The following simple Python program shows how Python handle text as list objects, itself already an excellent tokenization tool in NLP!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b36735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world. How are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784e55fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world.', 'How', 'are', 'you?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904469f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello world. How are you?' contains  5  words.\n"
     ]
    }
   ],
   "source": [
    "num_words = len(text.split())\n",
    "print (\"'Hello world. How are you?' contains \",num_words,\" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2110326",
   "metadata": {},
   "source": [
    "#### The following Python codes show how easy for Python to count the number of words from the famous novel \"Alice’s Adventures in Wonderland by Lewis Carroll\" as compared with other languages such as C or Java. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file alice.txt has about 29465 words.\n"
     ]
    }
   ],
   "source": [
    "# Python program to count the number of words the Free Books of Project Gutenberg https://www.gutenberg.org/\n",
    "def count_words(filename):\n",
    "    \"\"\"Count the approximate number of words in a file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, encoding='utf-8') as f_obj:\n",
    "            contents = f_obj.read() \n",
    "    except FileNotFoundError:\n",
    "        msg = \"Sorry, the file \" + filename + \" does not exist.\"\n",
    "        print(msg)\n",
    "    else:\n",
    "        # Count approximate number of words in the file.\n",
    "        words = contents.split()\n",
    "        num_words = len(words)\n",
    "        print(\"The file \" + filename + \" has about \" + str(num_words) + \" words.\")\n",
    "\n",
    "filename = 'alice.txt'\n",
    "count_words(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cdd28",
   "metadata": {},
   "source": [
    "In this course, I have extracted four famous classics from Project Gutenberg for you to try, they are:\n",
    "1. Alice's Adventures in Wonderland by Lewis Carroll (alice.txt)\n",
    "2. Little Women by Louisa May Alcott (little_women.txt)\n",
    "3. Moby Dick by Herman Melville (moby_dick.txt)\n",
    "4. The Adventures of Sherlock Holmes by Arthur Conan Doyle (Adventures_Holmes.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad40ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Adventures_Holmes.txt has about 107411 words.\n"
     ]
    }
   ],
   "source": [
    "count_words('Adventures_Holmes.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e744f8",
   "metadata": {},
   "source": [
    "## 10.5. NLTK with Basic Text Processing in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984d234",
   "metadata": {},
   "source": [
    "As said, one important feature NLTK is the provision of simple Python tools and methods for us to learn and practise NLP technology, which started by Basic Text Processing in NLP. \n",
    "They include:\n",
    "1. Basic text processing as lists of words.\n",
    "2. Basic statistics on text processing in NLP. \n",
    "3. Simple text analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52717fc",
   "metadata": {},
   "source": [
    "Before we start, of course we need to use some text document to start with. \n",
    "Just like Project Gutenburg word counting we have just learnt, nothing is more straight forward than start with analyzing the classics literature such as Moby Dick. \n",
    "However, in terms of NLP, it is even much better if we can study the text analysis of a variety of document types, such as classics, news and articles and even public speeches. \n",
    "Why? ....\n",
    "So in NLTK, it provides NINE typical text documents for us to start with. It contains: classic literatures, bible texts, famous public speeches, news and articles, and personal corpus. \n",
    "So, let's start ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805eb4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# Let's load some sample books from the nltk databank\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03118df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# To shows the list of books in NLTK books, try this:\n",
    "texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af56ff8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check only one book, e.g. text1, try this:\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bce5066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to know more about text1, try text1?\n",
    "text1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecc2f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "fholmes = open(\"Adventures_Holmes.txt\",\"r\",encoding=\"utf-8\").read()\n",
    "tokens = word_tokenize(fholmes)\n",
    "text=nltk.text.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96280218",
   "metadata": {},
   "source": [
    "## 10.6 Simple Text Analysis using \"concordance()\" , \"similar()\" and \"common_texts()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e115b04",
   "metadata": {},
   "source": [
    "In text analysis, one common operation is to study how a particular word (or phrase) appeared in a text document, especially in a classical and famous literature and text document such as public speeches. \n",
    "Different from normal \"search\" function, \"concordance()\" (索引) function allows us to study and analyze how a particular appeared within a text document. In other words, it not only shows the occurence, but more importantly the \"neighbouring words and phrases\" as well. \n",
    "Let's try some example in Adventures Holmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3da8880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 98 matches:\n",
      "﻿The Adventures of Sherlock Holmes by Arthur Conan Doyle Conte\n",
      "es I . A SCANDAL IN BOHEMIA I . To Sherlock Holmes she is always _the_ woman .\n",
      "ust such as I had pictured it from Sherlock Holmes ’ succinct description , bu\n",
      "ssing said : “ Good-night , Mister Sherlock Holmes. ” There were several peopl\n",
      "lly got it ! ” he cried , grasping Sherlock Holmes by either shoulder and look\n",
      "stepped from the brougham . “ Mr . Sherlock Holmes , I believe ? ” said she . \n",
      "ss for the Continent. ” “ What ! ” Sherlock Holmes staggered back , white with\n",
      ", the letter was superscribed to “ Sherlock Holmes , Esq . To be left till cal\n",
      "nd ran in this way : “ MY DEAR MR. SHERLOCK HOLMES , —You really did it very w\n",
      " of interest to the celebrated Mr. Sherlock Holmes . Then I , rather imprudent\n",
      " possess ; and I remain , dear Mr. Sherlock Holmes , “ Very truly yours , “ IR\n",
      "ia , and how the best plans of Mr. Sherlock Holmes were beaten by a woman ’ s \n",
      " I had called upon my friend , Mr. Sherlock Holmes , one day in the autumn of \n",
      "and discontent upon his features . Sherlock Holmes ’ quick eye took in my occu\n",
      "t as I have been telling you , Mr. Sherlock Holmes , ” said Jabez Wilson , mop\n",
      "e of this obliging youth ? ” asked Sherlock Holmes . “ His name is Vincent Spa\n",
      "IS DISSOLVED . October 9 , 1890. ” Sherlock Holmes and I surveyed this curt an\n",
      "d client carried on his business . Sherlock Holmes stopped in front of it with\n",
      " own stupidity in my dealings with Sherlock Holmes . Here I had heard what he \n",
      "” “ I think you will find , ” said Sherlock Holmes , “ that you will play for \n",
      "and I will follow in the second. ” Sherlock Holmes was not very communicative \n",
      "jump , and I ’ ll swing for it ! ” Sherlock Holmes had sprung out and seized t\n",
      "IDENTITY “ My dear fellow , ” said Sherlock Holmes as we sat on either side of\n",
      "ant-man behind a tiny pilot boat . Sherlock Holmes welcomed her with the easy \n",
      "nsult me in such a hurry ? ” asked Sherlock Holmes , with his finger-tips toge\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"Sherlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebeaf2",
   "metadata": {},
   "source": [
    "The above example shows all the occurrence of \"Sherlock\" inside the text document, by that we will study when it will be used. As one can see, as \"Sherlock\" is a rather \"special\" word with is strongly linked with the name \"Sherlock Holmes\", so almost all the time \"Sherlock\" and \"Holmes\" will be appeared together. \n",
    "\n",
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "However in terms of text analysis and especially for the learning of English from some great literature such as Adventures of Sherlock Holmes. \n",
    "Of course, one important thing we want to know to how some commonly-used words and phrases are used by these great authors and what other words of similar meanings (i.e. Synonyms) are being used to improve the Use of English. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28b084",
   "metadata": {},
   "source": [
    "In the following example, let's study how \"extreme\" is used in Adventures of Sherlock Holmes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e62c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 9 of 9 matches:\n",
      "may trust with a matter of the most extreme importance . If not , I should much\n",
      "ng red head , and the expression of extreme chagrin and discontent upon his fea\n",
      "ternately asserted itself , and his extreme exactness and astuteness represente\n",
      "e swing of his nature took him from extreme languor to devouring energy ; and ,\n",
      "olice reports realism pushed to its extreme limits , and yet the result is , it\n",
      "of an English provincial town . His extreme love of solitude in England suggest\n",
      "ion , and that in his haste and the extreme darkness he missed his path and wal\n",
      "for my coming at midnight , and his extreme anxiety lest I should tell anyone o\n",
      "like one who has been driven to the extreme limits of his reason . Then , sudde\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"extreme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437dd1b",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "Note: As one can see, in many dictionaries,we are using concordance technique to learn English, so-called \"Use of English\", which is not only on the grammatic aspect, but rather how different words (or phrases) are being used. As what we now called \"Learn by Examples\".\n",
    "In this example, we learnt how to use the word \"extreme\" in various situations and scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3dc3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense gathering\n"
     ]
    }
   ],
   "source": [
    "text.similar(\"extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c82e26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      "n another day or two perhaps ; this extreme mildness can hardly last longer -- \n",
      "ng her that he was kept away by the extreme affection for herself , which he co\n",
      " of his brother , and lamenting the extreme GAUCHERIE which he really believed \n",
      "y which had been leading her to the extreme of languid indolence and selfish re\n"
     ]
    }
   ],
   "source": [
    "text2.concordance(\"extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00f5966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family centre good opinion life death loss house society children\n",
      "attachment wishes interest goodness heart comfort cheerfulness\n",
      "existence marriage son\n"
     ]
    }
   ],
   "source": [
    "text2.similar(\"extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74705d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      " vigilance no Administration by any extreme of wickedness or folly can very ser\n",
      "ent , and communication between the extreme limits of the country made easier t\n",
      "the politics of petty bickering and extreme partisanship they plainly deplore .\n"
     ]
    }
   ],
   "source": [
    "text4.concordance(\"extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "580ae39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one other just hope motives act people agency system right form loss\n",
      "length knowledge science portion quarter narrowest requisite member\n"
     ]
    }
   ],
   "source": [
    "text4.similar(\"extreme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc001206",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "Note: As one can see, even a commonly used word \"extreme\" different people have differnt \"style\" of usage. \n",
    "In short, Herman Melville used the word \"extreme\"quite frequently in his literature and each with different style of usage. \n",
    "Jane Austen's usage of \"extreme\" is also very \"colorful\" and \"fruitful\", but not as frequently as Herman Melville. \n",
    "While in the Inaugural Address Corpus, the usage of word \"extreme\" become more \"standard\" and \"rigid\" in some sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d668c5a",
   "metadata": {},
   "source": [
    "The common_contexts() method allows you to examine the contexts that are shared by two or more words.\n",
    "\n",
    "Let's take a look on how it works.\n",
    "\n",
    "First, use Micky Dicky as example and try what is the common context for the two words: \"extreme\" and \"huge\".\n",
    "\n",
    "To do so, call the common_contexts() function from object text1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8433e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No common contexts were found\n"
     ]
    }
   ],
   "source": [
    "text.common_contexts([\"extreme\",\"huge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700a8be",
   "metadata": {},
   "source": [
    "What it meant is that: After analysing the two words \"extreme\" and \"huge\", it find out that the common context(s) for the usage of these two words is the \"pattern\" of: the_lower. \n",
    "\n",
    "To check it, what you can do is to call concordance() function for these two words and check against the patterns it extracted. As below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f136663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 9 of 9 matches:\n",
      "may trust with a matter of the most extreme importance . If not , I should much\n",
      "ng red head , and the expression of extreme chagrin and discontent upon his fea\n",
      "ternately asserted itself , and his extreme exactness and astuteness represente\n",
      "e swing of his nature took him from extreme languor to devouring energy ; and ,\n",
      "olice reports realism pushed to its extreme limits , and yet the result is , it\n",
      "of an English provincial town . His extreme love of solitude in England suggest\n",
      "ion , and that in his haste and the extreme darkness he missed his path and wal\n",
      "for my coming at midnight , and his extreme anxiety lest I should tell anyone o\n",
      "like one who has been driven to the extreme limits of his reason . Then , sudde\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f4a3dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "used and refreshed his memory with a huge pinch of snuff . “ Pray continue you\n",
      " after opening a third door , into a huge vault or cellar , which was piled al\n",
      "ed . All will come well . There is a huge error which it may take some little \n",
      " a small , office-like room , with a huge ledger upon the table , and a teleph\n",
      "en suddenly dashed open , and that a huge man had framed himself in the apertu\n",
      " , and bent it into a curve with his huge brown hands . “ See that you keep yo\n",
      "r. Grimesby Roylott drive past , his huge form looming up beside the little fi\n",
      "side and lay listless , watching the huge crest and monogram upon the envelope\n",
      " , ” said I ruefully , pointing to a huge bundle in the corner . “ I have had \n",
      "th hanging jowl , black muzzle , and huge projecting bones . It walked slowly \n",
      "r hurrying behind us . There was the huge famished brute , its black muzzle bu\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"huge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8abc87",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "Can you see it?\n",
    "\n",
    "How important it is in: \n",
    "1. NLP\n",
    "2. Use of English (and Technical Writing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da748d",
   "metadata": {},
   "source": [
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.1\n",
    "1. Try to use concordance(), similar() and common_contexts() function to look for the usage of TWO more commonly used words.\n",
    "2. Compare their usage in four different sources: 1) Moby Dick, 2) Sense and Sensibility, 3) Inaugural Address Corpus, and 4) Wall Street Journal.\n",
    "3. Any pattern(s) you can see?\n",
    "4. How different they are in terms of Use of English?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f1b9f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Workshop 1.1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa93e15",
   "metadata": {},
   "source": [
    "## 10.7 Text Analysis using Lexcial Dispersion Plot in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c65f1f",
   "metadata": {},
   "source": [
    "In the previous workshop we analysis the text documents by means of the word pattern and common contexts. \n",
    "\n",
    "How about the frequency of the location of the appearance of some \"key-words\" during the whole document?\n",
    "\n",
    "In NLTK in Python, we can apply the so-called Dispersion Plot. \n",
    "\n",
    "### 10.7.1 What is a Lexcial Dispersion Plot?\n",
    "\n",
    "In basic statistics, \"Dispersion\" is the quantification of deviation of each point from the mean value. \n",
    "\n",
    "NLTK Dispersion Plot produce a plot showing the distribution of the words through the text.\n",
    "\n",
    "Lexical dispersion illustrates the homogeneity of a word (or set of words) across the documents of a corpus. DispersionPlot allows for visualization of the lexical dispersion of words in a corpus. This plot illustrates with vertical lines the occurrences of one or more search terms throughout the corpus, noting how many words relative to the beginning of the corpus it appears.\n",
    "\n",
    "To invoke NLTK dispersion_plot, simple use the NLTK book object to call the function dispersion_plot().\n",
    "\n",
    "Note: To do the plot, \"pylab\" is needed to be installed.\n",
    "\n",
    "The following example try to use text1 to check the basic information about dispersion_plot(). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68a421b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.dispersion_plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2281501",
   "metadata": {},
   "source": [
    "### 10.7.2 Lexical Dispersion Plot over CONTEXT using Sense and Sensibility\n",
    "\n",
    "In term of literature writing one might wonder: Is there any lexcial pattern for using \"positive-thinking\" words such as \"good\". \"happy\" and \"strong\" verse the usage of \"negtive-thinking\" words such as \"bad\", \"sad\" or \"weak\". \n",
    "\n",
    "If it is the case, why?\n",
    "\n",
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.2\n",
    "In the following workshop, we try to analyse the classical English literature Sense and Sensibility by Jane Austen against these \"key-words\" to see if there is any lexical pattern arise. \n",
    "1. Use dispersion_plot to plot the Lexical Dispersion Plot of these key-words: good, happy, strong, bad, sad and weak. \n",
    "2. Study for any lexical pattern between the +ve verse -ve keywords. If it exist. WHY?\n",
    "3. Check these pattern against other classical literature such as Moby Dick to see if this pattern also happen and give your observations and justification. \n",
    "4. Add other TWO \"emotion keywords\" to see if the pattern still valid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d29ea9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2.dispersion_plot([\"good\",\"happy\",\"strong\",\"bad\",\"sad\",\"weak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579781aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Workshop 1.2 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4b3e2",
   "metadata": {},
   "source": [
    "### 10.7.3 Lexical Dispersion Plot over TIME using Inaugural Address Corpus\n",
    "\n",
    "One important thing about lexical usage is the changes over time. Which meant we would like to investigate is there any changes of lexical usage in written English (says) during the past 200 years?\n",
    "\n",
    "In order to do so, we need to look for a text document that contains text information that are changed over time during the whole document. \n",
    "\n",
    "The truth is: we have that text document in the NLTK book library -- The Inaugural Address Corpus \n",
    "\n",
    "The Inaugral Address Corpus contains the US presidents' inaugral address notes in the past 220 years. One important thing we would like to study is there any changes of lexical dispersion plot patterns for some key-words such as \"war\", \"peace\", \"freedom\" and \"united\" are noted during the past 220 years. If yes, why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb1e7c",
   "metadata": {},
   "source": [
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.3\n",
    "In the following workshop, we try to use 8 keywords and check whether there is any \"changes in lexical pattern\" noted during the past 220 years, these keywords are: America, citizens, democracy, freedom, war, peace, equal, united.\n",
    "1. Use dispersion_plot to invoke the Lexical Dispersion Plot for text4 (Inaugural Address Corpus).\n",
    "2. Study for any \"changes in lexical pattern\" in the past 220 years. \n",
    "3. Give any thoughts and explanation why that happens. \n",
    "4. Give ANY two useful keywords and check if they have the same changes in lexical pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5264d204",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text4.dispersion_plot([\"America\",\"citizens\",\"democracy\",\"freedom\",\"war\",\"peace\",\"equal\",\"united\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f73ef45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Workshop 1.3 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6ea5c",
   "metadata": {},
   "source": [
    "## 10.8 Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb8b80",
   "metadata": {},
   "source": [
    "### 10.8.1 What is Tokenization in NLP?\n",
    "\n",
    "Token is the most fundamental concept in NLP. \n",
    "\n",
    "A token may be a word, part of a word or just characters like punctuation. \n",
    "It is one of the most foundational NLP task and a difficult one, because every language has its own grammatical constructs, which are often difficult to write down as rules. \n",
    "\n",
    "Tokenization is the process of breaking down the documents or sentences into chunks called tokens. These tokens are mostly words, characters, or numbers but they can also be extended to include punctuation marks, symbols, and at times, understandable emotions.\n",
    "\n",
    "This is the first step we need to take to build a vocabulary. If you are wondering what a vocabulary means in the context of NLP, then a vocabulary is nothing but a set of unique words in our documents. So a vocabulary will consist of all the words present in our document without any repetition of words.\n",
    "\n",
    "The following figure shows the tokenization of the sentences: Jane lends $100 to Peter early this morning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f0435",
   "metadata": {},
   "source": [
    "<img src=\"./Fig10.5.png\" width = \"\" height = \"\" alt=\"note\" align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5fe00",
   "metadata": {},
   "source": [
    "NLTK provides an easy way to tokenize any string (of Text) by using tokenize() function. \n",
    "\n",
    "As shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "809c0652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane', 'lend', '$', '100', 'to', 'Peter', 'early', 'this', 'morning', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Jane lend $100 to Peter early this morning.'\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd0dc7",
   "metadata": {},
   "source": [
    "### 10.8.2 Different between Tokenize() vs Split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da371224",
   "metadata": {},
   "source": [
    "Reall in Section 4.1 that Python provides split() function to split a sentence of text into words. Let's see how it works as compared with the Tokenize() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a488eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane', 'lend', '$100', 'to', 'Peter', 'early', 'this', 'morning.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Jane lend $100 to Peter early this morning.'\n",
    "words = sent.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09413c68",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "Why they are difference? \n",
    "\n",
    "How important it is:\n",
    "1. In terms of NLP?\n",
    "2. In terms of Meaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efcb9e",
   "metadata": {},
   "source": [
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.4 Tokenization of The Adventures of Sherlock Holmes \n",
    "Although NLTK provides a list of great books and documents for us to try. Something we would like to explore other documents and literature to see for document analysis. \n",
    "In Section 4.4., we learnt a simple Python program to read and count words from any text documents. \n",
    "In this exercise, we try to use the famous novel \"The Adventures of Sherlock Holmes\" for document tokenization:\n",
    "Based on Python code shown in Section 4.4, write a Ptyhon program to:\n",
    "1. Read the Adventures_Holmes.txt text file. \n",
    "2. Save the content into a string object \"holmes_doc\".\n",
    "3. Use split() to cut it into list object \"holmes\".\n",
    "4. Count the total number of words inside the document. \n",
    "5. Tokenize the document by using NLTK tokenize() function.\n",
    "6. Count the total number of tokens.\n",
    "7. Compare the two figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae213379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Adventures_Holmes has about 107411 words.\n"
     ]
    }
   ],
   "source": [
    "# Workshop 4.4 Solution\n",
    "\n",
    "with open('Adventures_Holmes.txt', encoding='utf-8') as f_obj:\n",
    "    holmes_doc = f_obj.read() \n",
    "    # Count approximate number of words in the file.\n",
    "    holmes = holmes_doc.split()\n",
    "    num_words = len(holmes)\n",
    "    print(\"The file Adventures_Holmes has about \" + str(num_words) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b09924",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "NLTK provides a straight-forward way to count the total number of tokens inside a Text Document by using len() in NLTK package. \n",
    "\n",
    "Try len(text1) you will see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dddf115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128366"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0506b",
   "metadata": {},
   "source": [
    "### 10.8.3 Count Distinct Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb2b25",
   "metadata": {},
   "source": [
    "One important feature in text analysis is to study how many distinct words (so-called \"vocabulary\") appeared inside a text document. \n",
    "\n",
    "The truth is, once we can tokenize the whole text document as token objects. Python provide an easy way to group it into a set of distinct object by using Set() method. \n",
    "\n",
    "Set() is an useful method in Python to extract distinct objects (of any types) from a list of objects with repeated instances. \n",
    "\n",
    "Try the following by using Moby Dick you will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d350c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94281063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128366"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77d24d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heirs',\n",
       " 'abnormally',\n",
       " 'swelled',\n",
       " 'giving',\n",
       " 'threaten',\n",
       " 'silver.',\n",
       " 'decision',\n",
       " 'clients',\n",
       " 'pocket',\n",
       " 'love',\n",
       " 'streaked',\n",
       " 'disconnected',\n",
       " 'attain',\n",
       " 'neighbours',\n",
       " 'opal',\n",
       " 'lost.',\n",
       " 'huddled',\n",
       " 'piteous',\n",
       " 'hours.',\n",
       " 'ago—to',\n",
       " 'lad.',\n",
       " 'Union',\n",
       " 'probability—the',\n",
       " 'swear',\n",
       " 'curves',\n",
       " 'low',\n",
       " 'private',\n",
       " 'chewing',\n",
       " 'was—',\n",
       " 'prying',\n",
       " 'legal',\n",
       " 'Paddington',\n",
       " 'lines',\n",
       " 'boards',\n",
       " 'gem',\n",
       " 'illness',\n",
       " 'unfenced',\n",
       " 'Young',\n",
       " 'required',\n",
       " 'glimmer',\n",
       " 'appointment.',\n",
       " 'benefactor',\n",
       " 'FOUNDATION',\n",
       " 'perch',\n",
       " 'rambling',\n",
       " 'detailed',\n",
       " 'compressed',\n",
       " 'crackling',\n",
       " 'dried',\n",
       " 'agrees',\n",
       " 'imagination',\n",
       " 'him.',\n",
       " 'fathom.',\n",
       " 'searched',\n",
       " 'specialist',\n",
       " 'Holborn',\n",
       " 'source',\n",
       " 'law-abiding',\n",
       " 'disease',\n",
       " 'side.',\n",
       " 'origin.',\n",
       " 'CONTRACT',\n",
       " 'forwarded',\n",
       " 'Would',\n",
       " 'weight',\n",
       " 'simplicity',\n",
       " 'wander.',\n",
       " 'shook',\n",
       " 'proofread',\n",
       " 'and',\n",
       " 'recognise',\n",
       " 'inquirer',\n",
       " 'himself.',\n",
       " 'bell-rope',\n",
       " 'banged',\n",
       " 'begin',\n",
       " 'forced',\n",
       " 'frighten',\n",
       " 'ball.',\n",
       " 'less',\n",
       " 'descended',\n",
       " 'splendour',\n",
       " 'venomous',\n",
       " 'calamity',\n",
       " 'cabinet',\n",
       " 'does',\n",
       " 'statement',\n",
       " 'waylaid',\n",
       " 'wanting',\n",
       " 'F.',\n",
       " 'established',\n",
       " 'trimly',\n",
       " 'quavering',\n",
       " 'awake',\n",
       " 'Warburton',\n",
       " 'abide',\n",
       " 'handled',\n",
       " 'noiseless',\n",
       " 'gummed',\n",
       " 'preserved',\n",
       " 'deathbeds',\n",
       " 'injections',\n",
       " 'again',\n",
       " 'Germans',\n",
       " 'august',\n",
       " 'flock.',\n",
       " 'Stoner',\n",
       " 'used',\n",
       " 'pince-nez',\n",
       " 'stable',\n",
       " 'succeeded',\n",
       " 'geology',\n",
       " 'eight-and-forty',\n",
       " 'trembling',\n",
       " 'convulsive',\n",
       " 'blush',\n",
       " ':',\n",
       " 'clears',\n",
       " 'gainer',\n",
       " 'pain',\n",
       " 'nominal',\n",
       " 'bearded',\n",
       " 'power.',\n",
       " 'repented',\n",
       " 'improvisations',\n",
       " 'revolver',\n",
       " 'garden',\n",
       " 'ornament',\n",
       " 'unprofitable.',\n",
       " 'now',\n",
       " 'sheep',\n",
       " 'introduction',\n",
       " 'Mauritius',\n",
       " 'blur',\n",
       " 'correspondent',\n",
       " 'expression',\n",
       " 'stillness',\n",
       " 'roofs',\n",
       " 'hawk-like',\n",
       " 'myth',\n",
       " 'scraping',\n",
       " '1869',\n",
       " 'possible',\n",
       " 'machine-readable',\n",
       " 'caps',\n",
       " 'stamped',\n",
       " 'rat',\n",
       " 'also—but',\n",
       " 'Lee',\n",
       " 'hands—',\n",
       " 'tear',\n",
       " 'evidently',\n",
       " 'cloth',\n",
       " 'happening',\n",
       " 'slashed',\n",
       " 'deuce',\n",
       " 'any',\n",
       " 'rifts',\n",
       " 'harsh',\n",
       " 'darting',\n",
       " 'seem',\n",
       " 'base',\n",
       " 'answered',\n",
       " 'foolscap',\n",
       " 'heap',\n",
       " 'Secretary',\n",
       " 'failing',\n",
       " 'drowned',\n",
       " 'splendid',\n",
       " 'ridiculously',\n",
       " 'paleness',\n",
       " '8_s_.',\n",
       " 'tallish',\n",
       " 'Stop',\n",
       " 'footing',\n",
       " 'consideration.',\n",
       " 'bare',\n",
       " 'Valley',\n",
       " 'docketing',\n",
       " 'wasted.',\n",
       " 'farms',\n",
       " 'fraud',\n",
       " 'dim',\n",
       " 'framework',\n",
       " 'Eton',\n",
       " 'England—a',\n",
       " 'mahogany',\n",
       " 'sunbeam',\n",
       " 'parish',\n",
       " 'spark',\n",
       " 'tugging',\n",
       " 'unique.',\n",
       " 'Florida',\n",
       " 'curse',\n",
       " 'straighten',\n",
       " 'eagerness',\n",
       " 'acknowledges',\n",
       " 'throat',\n",
       " 'nails',\n",
       " 'restive',\n",
       " 'duties',\n",
       " 'chalk',\n",
       " 'AND',\n",
       " 'Adventure',\n",
       " 'inflicted',\n",
       " 'invention',\n",
       " 'due',\n",
       " 'week.',\n",
       " 'acts',\n",
       " 'preoccupied',\n",
       " 'ash',\n",
       " 'Saviour',\n",
       " 'note-book',\n",
       " 'fast',\n",
       " 'sell',\n",
       " 'subdued',\n",
       " 'Thank',\n",
       " 'confirmed',\n",
       " 'results.',\n",
       " 'obstacle',\n",
       " 'notepaper',\n",
       " 'Office',\n",
       " 'nearing',\n",
       " 'open-eyed',\n",
       " 'exciting',\n",
       " 'Half',\n",
       " 'ceiling',\n",
       " 'landowner',\n",
       " 'Alas',\n",
       " 'exit',\n",
       " 'readily',\n",
       " 'trunks',\n",
       " 'STRICT',\n",
       " 'crisp',\n",
       " 'excitable',\n",
       " 'devouring.',\n",
       " 'eBooks',\n",
       " 'communication',\n",
       " 'serenely',\n",
       " 'blotted',\n",
       " 'over-pleasant',\n",
       " 'artificial',\n",
       " 'sweetness',\n",
       " 'rearranging',\n",
       " 'shivering.',\n",
       " 'circle',\n",
       " 'times.',\n",
       " 'grateful',\n",
       " 'footsteps',\n",
       " 'chinchilla',\n",
       " '9',\n",
       " 'opium-smoking',\n",
       " 'illegally',\n",
       " 'lovely',\n",
       " 'erect',\n",
       " 'bless',\n",
       " 'destined',\n",
       " 'blunders',\n",
       " 'ulster',\n",
       " 'jealousy',\n",
       " 'WILL',\n",
       " 'breastpin.',\n",
       " 'stripped',\n",
       " 'refuse.',\n",
       " 'kind.',\n",
       " 'sensations',\n",
       " 'Carbuncle',\n",
       " 'secreting',\n",
       " 'reports',\n",
       " 'remunerative',\n",
       " 'Bow',\n",
       " 'wintry',\n",
       " 'intruding',\n",
       " 'sally',\n",
       " 'lure',\n",
       " 'firemen',\n",
       " 'necessary',\n",
       " 'records',\n",
       " 'available',\n",
       " 'limb',\n",
       " 'good.',\n",
       " 'moonshine.',\n",
       " 'lumber-room.',\n",
       " 'transcribe',\n",
       " 'lawn',\n",
       " 'plain',\n",
       " 'strain',\n",
       " 'grow',\n",
       " 'disturbing',\n",
       " 'examined',\n",
       " 'qualifications',\n",
       " 'cobwebby',\n",
       " 'Living',\n",
       " 'trying',\n",
       " 'late—forever',\n",
       " '1.E.6',\n",
       " 'union',\n",
       " 'unpack',\n",
       " 'limped—he',\n",
       " 'gasfitters',\n",
       " 'Precisely',\n",
       " 'pardon.',\n",
       " 'prosperity',\n",
       " 'BAND',\n",
       " 'method',\n",
       " 'everywhere',\n",
       " 'leaped',\n",
       " 'sounded',\n",
       " 'hundred',\n",
       " 'sweet',\n",
       " 'happens',\n",
       " 'dowry',\n",
       " 'fasteners',\n",
       " 'secured',\n",
       " 'side-whiskers',\n",
       " 'man—a',\n",
       " 'intrusted',\n",
       " 'precaution.',\n",
       " 'treat',\n",
       " 'Encyclopædia_',\n",
       " 'plans.',\n",
       " 'two',\n",
       " 'Come',\n",
       " 'America',\n",
       " 'Aldershot',\n",
       " 'generations',\n",
       " 'inquired',\n",
       " 'scene.',\n",
       " 'soul.',\n",
       " 'Red-headed',\n",
       " 'Pool',\n",
       " 'Hunter.',\n",
       " 'earnest',\n",
       " 'corridors',\n",
       " 'Remember',\n",
       " 'regards',\n",
       " 'experienced',\n",
       " 'London.',\n",
       " 'smell',\n",
       " 'specimen',\n",
       " 'boxer',\n",
       " 'communicated',\n",
       " 'Everybody',\n",
       " '_Encyclopædia_',\n",
       " 'ground—',\n",
       " 'isn',\n",
       " 'Americans',\n",
       " 'Apache',\n",
       " 'bridge',\n",
       " 'region',\n",
       " 'Send',\n",
       " 'programme',\n",
       " 'seat.',\n",
       " 'grinned',\n",
       " 'gravel',\n",
       " 'Cassel-Felstein',\n",
       " 'shoulder',\n",
       " 'companies',\n",
       " 'unfortunate',\n",
       " 'lace',\n",
       " 'unseat',\n",
       " 'free',\n",
       " 'search.',\n",
       " 'version',\n",
       " \"Gutenberg-tm's\",\n",
       " 'stage—ha',\n",
       " 'sin',\n",
       " 'ring—',\n",
       " 'These',\n",
       " 'huge',\n",
       " 'lanes',\n",
       " 'carts',\n",
       " 'Contributions',\n",
       " 'sobered',\n",
       " 'revenge',\n",
       " 'Daily',\n",
       " 'visitor',\n",
       " 'Yours',\n",
       " 'wincing',\n",
       " 'innocent',\n",
       " 'girl.',\n",
       " '1890.',\n",
       " 'seal.',\n",
       " 'Moulton',\n",
       " 'associate',\n",
       " 'knocked',\n",
       " 'Don',\n",
       " 'day—it',\n",
       " 'spent',\n",
       " 'silent',\n",
       " 'becoming',\n",
       " '220',\n",
       " 'Klan',\n",
       " 'probable.',\n",
       " 'consoled',\n",
       " 'slippery',\n",
       " 'Boscombe',\n",
       " 'varieties',\n",
       " 'Cusack',\n",
       " 'unreasoning',\n",
       " 'Allegro',\n",
       " 'shops',\n",
       " 'standing',\n",
       " 'Imitated.',\n",
       " 'as',\n",
       " 'screening',\n",
       " 'effort',\n",
       " 'traces',\n",
       " 'self-evident',\n",
       " 'inquire',\n",
       " 'degenerating',\n",
       " 'approached',\n",
       " 'flushing',\n",
       " 'Marseilles',\n",
       " 'Hayling',\n",
       " 'points',\n",
       " 'slate-coloured',\n",
       " 'sketch',\n",
       " 'before-breakfast',\n",
       " 'Easier',\n",
       " 'jest',\n",
       " 'criminals',\n",
       " 'Etherege',\n",
       " 'LIABILITY',\n",
       " 'gallows',\n",
       " 'admiration',\n",
       " 'moved',\n",
       " 'talent',\n",
       " 'instrument',\n",
       " 'unclasping',\n",
       " 'adhesive',\n",
       " 'tendencies',\n",
       " 'Think',\n",
       " 'perplexing',\n",
       " 'staccato',\n",
       " 'loathed',\n",
       " 'informality',\n",
       " 'Arabian',\n",
       " 'Each',\n",
       " 'her',\n",
       " 'Have',\n",
       " 'pondered',\n",
       " 'lip',\n",
       " 'weary',\n",
       " 'meets',\n",
       " 'to',\n",
       " 'Frankly',\n",
       " 'staggering',\n",
       " 'Armour',\n",
       " 'more.',\n",
       " 'stale',\n",
       " 'factory',\n",
       " 'plenty',\n",
       " 'rate.',\n",
       " 'card',\n",
       " 'how',\n",
       " 'solicitor',\n",
       " 'weaken',\n",
       " 'seek',\n",
       " 'call.',\n",
       " 'match-seller',\n",
       " 'attempted',\n",
       " 'Gutenberg',\n",
       " 'lonelier',\n",
       " 'For',\n",
       " 'to-night',\n",
       " 'movement',\n",
       " 'gaslight',\n",
       " 'drooping',\n",
       " 'Papier.',\n",
       " 'baits',\n",
       " 'prejudice',\n",
       " 'goose',\n",
       " 'anxiety',\n",
       " 'deceived',\n",
       " 'bit',\n",
       " 'braved',\n",
       " '26_s_',\n",
       " '_née_',\n",
       " 'hoard',\n",
       " 'loss',\n",
       " 'roots',\n",
       " 'undated',\n",
       " 'severely',\n",
       " 'arranged',\n",
       " 'estate',\n",
       " 'frantic',\n",
       " 'supper.',\n",
       " 'over-clean',\n",
       " 'explanations',\n",
       " 'them.',\n",
       " 'unusually',\n",
       " 'snow-clad',\n",
       " '—you',\n",
       " 'necessity',\n",
       " 'stairs',\n",
       " 'path',\n",
       " 'tortured',\n",
       " 'plunge',\n",
       " 'carpenter.',\n",
       " 'grinning',\n",
       " 'inaccurate',\n",
       " 'thickly',\n",
       " 'mess',\n",
       " 'from',\n",
       " 'successful',\n",
       " 'stream',\n",
       " 'smokeless',\n",
       " 'Logic',\n",
       " 'hinted',\n",
       " 'vile',\n",
       " 'successes',\n",
       " 'absorbed',\n",
       " 'inferences.',\n",
       " 'Londoners',\n",
       " 'Eight',\n",
       " 'groping',\n",
       " 'why',\n",
       " 'shocked',\n",
       " 'shutters',\n",
       " 'Dear',\n",
       " 'prefer',\n",
       " 'couples',\n",
       " 'shouldn',\n",
       " 'guilty',\n",
       " 'inconsequential',\n",
       " 'extend',\n",
       " 'note-paper.',\n",
       " 'inside',\n",
       " 'system',\n",
       " 'recollect',\n",
       " 'Donations',\n",
       " 'altogether',\n",
       " 'carriage-sweep',\n",
       " 'engaged.',\n",
       " 'Above',\n",
       " 'deeply.',\n",
       " 'chair.',\n",
       " 'softly',\n",
       " 'Away',\n",
       " 'announced',\n",
       " 'Wigmore',\n",
       " 'copy',\n",
       " 'room—you',\n",
       " 'dozen.',\n",
       " 'stirring',\n",
       " 'island',\n",
       " 'hardest',\n",
       " 'proportion',\n",
       " 'American',\n",
       " 'James',\n",
       " 'reconsider',\n",
       " 'draughts',\n",
       " 'overstrung',\n",
       " 'this',\n",
       " 'seen—',\n",
       " 'arduous',\n",
       " 'ceases',\n",
       " 'alternation',\n",
       " 'News_',\n",
       " 'invited',\n",
       " 'spell',\n",
       " 'trough',\n",
       " 'half-hopeful',\n",
       " 'game-keeper',\n",
       " 'empty',\n",
       " 'injunction',\n",
       " 'radiance',\n",
       " 'horrify',\n",
       " 'coin',\n",
       " 'Imagine',\n",
       " 'garment',\n",
       " 'rumour',\n",
       " 'nonentity',\n",
       " 'wooing',\n",
       " 'Seven',\n",
       " 'virtue',\n",
       " 'ramblings',\n",
       " 'D',\n",
       " 'child—one',\n",
       " 'thoroughly',\n",
       " 'absorbing.',\n",
       " 'heads',\n",
       " 'laughing-stock',\n",
       " 'circles',\n",
       " 'catlike',\n",
       " 'linked',\n",
       " 'medium',\n",
       " 'abstracted',\n",
       " 'then—',\n",
       " 'received',\n",
       " 'rare',\n",
       " 'sleepy',\n",
       " 'threatened',\n",
       " 'thought',\n",
       " 'imagine.',\n",
       " 'secluded',\n",
       " 'prominence',\n",
       " 'printed',\n",
       " 'frequent',\n",
       " 'mad—insane.',\n",
       " 'enjoy',\n",
       " '25',\n",
       " 'Morris',\n",
       " 'data',\n",
       " 'thoughtfully',\n",
       " 'professional',\n",
       " 'caraffe',\n",
       " 'far',\n",
       " 'bleeding',\n",
       " 'membra_',\n",
       " 'done',\n",
       " '_disjecta',\n",
       " 'web',\n",
       " 'elapsed',\n",
       " 'Heavy',\n",
       " 'plucked',\n",
       " 'dangers',\n",
       " 'disappoint',\n",
       " 'mines.',\n",
       " 'reception',\n",
       " 'Archive',\n",
       " 'Chronicle_',\n",
       " 'false',\n",
       " 'French',\n",
       " 'accustomed',\n",
       " 'amused.',\n",
       " 'parted',\n",
       " 'exclamation',\n",
       " 'fantastic',\n",
       " 'volunteers',\n",
       " 'wears',\n",
       " 'forefingers',\n",
       " 'stabbed',\n",
       " 'fumbled',\n",
       " 'desire',\n",
       " 'blot',\n",
       " 'vice',\n",
       " 'firm',\n",
       " 'slovenly',\n",
       " 'absent',\n",
       " 'NOT',\n",
       " 'requirement',\n",
       " 'seated',\n",
       " 'sealed',\n",
       " 'ado',\n",
       " 'met',\n",
       " 'expound.',\n",
       " 'stopped',\n",
       " 'merely',\n",
       " 'akimbo',\n",
       " 'Hudson',\n",
       " 'Berkshire',\n",
       " 'watch-chain',\n",
       " 'blanche_',\n",
       " 'foreseen',\n",
       " 'to—none',\n",
       " 'wave',\n",
       " 'rose',\n",
       " 'tiny',\n",
       " 'different.',\n",
       " 'shimmering',\n",
       " 'carries',\n",
       " 'sure',\n",
       " 'energetic',\n",
       " 'tread',\n",
       " 'depot',\n",
       " 'boarding-school',\n",
       " 'temperament',\n",
       " 'rapidity',\n",
       " 'problems',\n",
       " 'span',\n",
       " 'person.',\n",
       " 'suite',\n",
       " 'little',\n",
       " 'devotedly',\n",
       " 'ways',\n",
       " 'creditor',\n",
       " 'equipment',\n",
       " 'subject.',\n",
       " 'sceptic',\n",
       " 'gave',\n",
       " 'nerve',\n",
       " 'resolution',\n",
       " 'ones',\n",
       " 'Amoy',\n",
       " 'brace',\n",
       " 'chamois',\n",
       " 'avert',\n",
       " 'town',\n",
       " 'writings',\n",
       " 'true.',\n",
       " 'rummaged',\n",
       " 'daring',\n",
       " 'trivial',\n",
       " 'tell',\n",
       " 'Cocksure',\n",
       " 'bedtime',\n",
       " 'distant',\n",
       " 'dropping',\n",
       " 'bound',\n",
       " 'implore',\n",
       " 'PG',\n",
       " 'skull',\n",
       " 'legal.',\n",
       " 'foie',\n",
       " 'particularly',\n",
       " 'drive.',\n",
       " 'Those',\n",
       " 'accused',\n",
       " 'ill-trimmed',\n",
       " 'result.',\n",
       " 'jury.',\n",
       " 'bulky',\n",
       " 'glance',\n",
       " 'heartily',\n",
       " 'LIABLE',\n",
       " 'Putting',\n",
       " 'Esq.',\n",
       " 'simple-minded',\n",
       " 'new',\n",
       " 'planned',\n",
       " 'photograph',\n",
       " 'arrangements',\n",
       " 'Clark',\n",
       " 'Philosophy',\n",
       " 'Friday',\n",
       " 'battered',\n",
       " 'imbecile',\n",
       " 'viewed',\n",
       " 'supply.',\n",
       " 'sheer',\n",
       " 'vacant',\n",
       " 'know.',\n",
       " 'forgotten',\n",
       " 'counties',\n",
       " 'recourse',\n",
       " 'kind',\n",
       " 'ominous',\n",
       " 'notorious',\n",
       " 'amused',\n",
       " 'well.',\n",
       " 'keys',\n",
       " 'promised',\n",
       " 'looking',\n",
       " 'bonnet',\n",
       " 'sole',\n",
       " 'compliment',\n",
       " 'stiff.',\n",
       " 'Ferguson',\n",
       " 'authority',\n",
       " 'depended',\n",
       " 'disappearance.',\n",
       " 'no',\n",
       " 'penetrating',\n",
       " 'lurking',\n",
       " 'Museum—we',\n",
       " 'details',\n",
       " 'higher',\n",
       " 'wiser',\n",
       " 'steel',\n",
       " 'spouting',\n",
       " 'octavo',\n",
       " 'catch',\n",
       " 'encouraging',\n",
       " 'MAN',\n",
       " 'Every',\n",
       " 'thresholds',\n",
       " 'lock',\n",
       " 'wiry',\n",
       " 'excuses',\n",
       " 'whine',\n",
       " 'yard',\n",
       " 'Deserted',\n",
       " 'liver',\n",
       " 'manage',\n",
       " 'destitute',\n",
       " '_St',\n",
       " 'dubious',\n",
       " 'gear',\n",
       " 'staggered',\n",
       " 'contemplation',\n",
       " 'COPPER',\n",
       " 'limps',\n",
       " 'astonished',\n",
       " 'YOU',\n",
       " 'Horsham',\n",
       " 'buying',\n",
       " 'represented',\n",
       " 'locations',\n",
       " 'diadem',\n",
       " 'REFUND',\n",
       " 'gazing',\n",
       " 'something',\n",
       " 'One',\n",
       " 'widened',\n",
       " 'illegal',\n",
       " 'exposed',\n",
       " 'sound',\n",
       " 'Lee.',\n",
       " 'sweating',\n",
       " 'mission',\n",
       " 'Count',\n",
       " 'Continent.',\n",
       " 'Yesterday.',\n",
       " 'anxiety.',\n",
       " 'chief',\n",
       " 'reading',\n",
       " 'Hum',\n",
       " 'lured',\n",
       " 'abrupt',\n",
       " 'pipes',\n",
       " 'oak-leaves',\n",
       " 'symptom',\n",
       " 'madly',\n",
       " 'authorities',\n",
       " 'MYSTERY',\n",
       " 'grace',\n",
       " 'gravity.',\n",
       " 'example.',\n",
       " 'compress',\n",
       " 'timbered',\n",
       " 'darkness—such',\n",
       " 'breakfast.',\n",
       " 'Gazette_',\n",
       " 'smoke',\n",
       " 'Assizes.',\n",
       " 'hopeless',\n",
       " 'bicycling',\n",
       " 'our',\n",
       " 'party',\n",
       " 'unimpeachable',\n",
       " 'Bordeaux',\n",
       " 'Regency',\n",
       " 'securing',\n",
       " 'has.',\n",
       " 'grin',\n",
       " 'strayed',\n",
       " 'talked',\n",
       " 'eccentricity',\n",
       " 'deeply',\n",
       " 'rolling',\n",
       " 'dressing-table.',\n",
       " 'deal',\n",
       " 'slipping',\n",
       " 'broadest',\n",
       " 'inquiry.',\n",
       " 'position.',\n",
       " '12_s_.',\n",
       " 'Plain',\n",
       " 'been',\n",
       " 'true',\n",
       " 'womanly',\n",
       " 'cross-purposes',\n",
       " 'boy',\n",
       " 'animated',\n",
       " 'Presently',\n",
       " 'cost',\n",
       " 'sufficient',\n",
       " 'breathlessly',\n",
       " 'heinous',\n",
       " 'hideous',\n",
       " 'laughing',\n",
       " 'disregarded',\n",
       " 'teach',\n",
       " 'particular',\n",
       " 'imposing',\n",
       " 'dusty',\n",
       " 'stooping',\n",
       " 'forgiveness',\n",
       " 'Pa',\n",
       " 'strikes',\n",
       " 'empire',\n",
       " 'romper',\n",
       " 'Snapping',\n",
       " 'Oct',\n",
       " 'above',\n",
       " 'bright-looking',\n",
       " 'pet',\n",
       " 'scribbled',\n",
       " 'license',\n",
       " 'devil',\n",
       " 'motion',\n",
       " 'corroboration',\n",
       " 'vanishing',\n",
       " 'once.',\n",
       " 'sign',\n",
       " 'whoso',\n",
       " 'bonniest',\n",
       " 'incites',\n",
       " 'scenes',\n",
       " 'prosperous',\n",
       " 'date.',\n",
       " 'farthest',\n",
       " 'crying',\n",
       " 'myself',\n",
       " 'horror-stricken',\n",
       " 'ACTUAL',\n",
       " 'Post_',\n",
       " 'rabbit',\n",
       " 'two-hundred-year-old',\n",
       " 'Instead',\n",
       " 'basket-chair',\n",
       " 'tap',\n",
       " 'finished',\n",
       " 'bright',\n",
       " 'occupation',\n",
       " 'faced',\n",
       " 'slow',\n",
       " 'Please',\n",
       " 'finding',\n",
       " 'crowd',\n",
       " 'throw',\n",
       " 'nose',\n",
       " '_outré_',\n",
       " 'Aldersgate',\n",
       " 'gasped',\n",
       " 'noise',\n",
       " 'bow',\n",
       " 'literature',\n",
       " 'Lancaster',\n",
       " 'year—which',\n",
       " 'risen',\n",
       " 'sundial.',\n",
       " 'fits',\n",
       " 'barometric',\n",
       " 'inferred',\n",
       " 'detailing',\n",
       " 'flash',\n",
       " 'soothingly',\n",
       " 'the',\n",
       " 'gain.',\n",
       " 'clutched',\n",
       " 'scrawled',\n",
       " 'Two',\n",
       " 'surest',\n",
       " 'Good-day',\n",
       " 'managed',\n",
       " 'post',\n",
       " 'distinctive.',\n",
       " 'Neither',\n",
       " 'noting',\n",
       " 'mole',\n",
       " 'Jem',\n",
       " 'sneer',\n",
       " 'dangling',\n",
       " 'hers—possibly',\n",
       " 'fact.',\n",
       " 'refer',\n",
       " 'death—',\n",
       " 'averse',\n",
       " 'recesses',\n",
       " 'colony',\n",
       " 'says',\n",
       " 'black-letter',\n",
       " 'concentrated',\n",
       " 'inspect',\n",
       " 'processing',\n",
       " 'die',\n",
       " 'attempting',\n",
       " 'so',\n",
       " 'g',\n",
       " 'remarks',\n",
       " 'obliged',\n",
       " 'lieu',\n",
       " '_Omne',\n",
       " 'shattered',\n",
       " 'presence—in',\n",
       " 'proceeded',\n",
       " 'earrings',\n",
       " 'importers',\n",
       " 'massive',\n",
       " 'repay',\n",
       " 'Victoria.',\n",
       " 'establishment',\n",
       " 'bunch',\n",
       " 'action',\n",
       " 'CASE',\n",
       " 'continue',\n",
       " 'puny',\n",
       " 'Anyhow',\n",
       " 'notice.',\n",
       " 'needle-work',\n",
       " 'agreed',\n",
       " 'ship.',\n",
       " 'amazement',\n",
       " 'concentration',\n",
       " 'shutting',\n",
       " 'worn',\n",
       " 'remembered',\n",
       " 'staring',\n",
       " 'examined.',\n",
       " 'expend',\n",
       " 'billycock',\n",
       " 'curving',\n",
       " 'knock',\n",
       " 'Circumstantial',\n",
       " 'Perfectly',\n",
       " 'contrary',\n",
       " 'builder',\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79706214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10048"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119ea69",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "From this example, you will see Moby Dick contains 260819 tokens (ie. words and punctuations). But actually, the total number of distinct tokens (so-called \"types\") is 19317. Still a very big number.\n",
    "Try other literature, you will see how great Moby Dick in terms of vocabulary you can learn by reading the book!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf4a81",
   "metadata": {},
   "source": [
    "The following example shows how to sort the distinct tokens by using sorted() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9cac3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '!\"',\n",
       " '!\"--',\n",
       " \"!'\",\n",
       " '!\\'\"',\n",
       " '!)',\n",
       " '!)\"',\n",
       " '!*',\n",
       " '!--',\n",
       " '!--\"',\n",
       " \"!--'\",\n",
       " '\"',\n",
       " '\"\\'',\n",
       " '\"--',\n",
       " '\"...',\n",
       " '\";',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " \"',\",\n",
       " \"',--\",\n",
       " \"'-\",\n",
       " \"'--\",\n",
       " \"';\",\n",
       " '(',\n",
       " ')',\n",
       " '),',\n",
       " ')--',\n",
       " ').',\n",
       " ').--',\n",
       " '):',\n",
       " ');',\n",
       " ');--',\n",
       " '*',\n",
       " ',',\n",
       " ',\"',\n",
       " ',\"--',\n",
       " \",'\",\n",
       " \",'--\",\n",
       " ',)',\n",
       " ',*',\n",
       " ',--',\n",
       " ',--\"',\n",
       " \",--'\",\n",
       " '-',\n",
       " '--',\n",
       " '--\"',\n",
       " \"--'\",\n",
       " '--\\'\"',\n",
       " '--(',\n",
       " '---\"',\n",
       " '---,',\n",
       " '.',\n",
       " '.\"',\n",
       " '.\"*',\n",
       " '.\"--',\n",
       " \".'\",\n",
       " '.\\'\"',\n",
       " '.)',\n",
       " '.*',\n",
       " '.*--',\n",
       " '.,',\n",
       " '.--',\n",
       " '.--\"',\n",
       " '...',\n",
       " '....',\n",
       " '.]',\n",
       " '000',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '109',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '113',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '12',\n",
       " '120',\n",
       " '121',\n",
       " '122',\n",
       " '123',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '127',\n",
       " '128',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '131',\n",
       " '132',\n",
       " '133',\n",
       " '134',\n",
       " '135',\n",
       " '14',\n",
       " '144',\n",
       " '1492',\n",
       " '15',\n",
       " '150',\n",
       " '15th',\n",
       " '16',\n",
       " '1652',\n",
       " '1668',\n",
       " '1671',\n",
       " '1690',\n",
       " '1695',\n",
       " '16th',\n",
       " '17',\n",
       " '1726',\n",
       " '1729',\n",
       " '1750',\n",
       " '1772',\n",
       " '1775',\n",
       " '1776',\n",
       " '1778',\n",
       " '1779',\n",
       " '1788',\n",
       " '1791',\n",
       " '1793',\n",
       " '18',\n",
       " '180',\n",
       " '1807',\n",
       " '1819',\n",
       " '1820',\n",
       " '1821',\n",
       " '1825',\n",
       " '1828',\n",
       " '1833',\n",
       " '1836',\n",
       " '1839',\n",
       " '1840',\n",
       " '1842',\n",
       " '1846',\n",
       " '1850',\n",
       " '1851',\n",
       " '19',\n",
       " '1ST',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '2000',\n",
       " '200th',\n",
       " '21',\n",
       " '21st',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '275th',\n",
       " '28',\n",
       " '29',\n",
       " '2ND',\n",
       " '3',\n",
       " '30',\n",
       " '31',\n",
       " '31st',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '37',\n",
       " '38',\n",
       " '39',\n",
       " '3D',\n",
       " '3d',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '440',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '49',\n",
       " '4TH',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '550',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '59',\n",
       " '5TH',\n",
       " '6',\n",
       " '60',\n",
       " '61',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69',\n",
       " '7',\n",
       " '70',\n",
       " '71',\n",
       " '72',\n",
       " '73',\n",
       " '74',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '8',\n",
       " '80',\n",
       " '800',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '890',\n",
       " '9',\n",
       " '90',\n",
       " '91',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " ':',\n",
       " ':\"-',\n",
       " ':--',\n",
       " ':--\"',\n",
       " \":--'\",\n",
       " ';',\n",
       " ';\"',\n",
       " ';\"--',\n",
       " ';\"--(',\n",
       " \";'\",\n",
       " ';*',\n",
       " ';--',\n",
       " ';--\"',\n",
       " \";--'\",\n",
       " '?',\n",
       " '?\"',\n",
       " '?\"--',\n",
       " \"?'\",\n",
       " \"?'--'\",\n",
       " '?--',\n",
       " '?--\"',\n",
       " \"?--'\",\n",
       " 'A',\n",
       " 'ABOUT',\n",
       " 'ACCOUNT',\n",
       " 'ADDITIONAL',\n",
       " 'ADVANCING',\n",
       " 'ADVENTURES',\n",
       " 'AFFGHANISTAN',\n",
       " 'AFRICA',\n",
       " 'AFTER',\n",
       " 'AGAINST',\n",
       " 'AHAB',\n",
       " 'ALFRED',\n",
       " 'ALGERINE',\n",
       " 'ALIVE',\n",
       " 'ALL',\n",
       " 'ALONE',\n",
       " 'AM',\n",
       " 'AMERICA',\n",
       " 'AMONG',\n",
       " 'ANCHORS',\n",
       " 'AND',\n",
       " 'ANGLO',\n",
       " 'ANIMAL',\n",
       " 'ANNALS',\n",
       " 'ANNUS',\n",
       " 'ANOTHER',\n",
       " 'ANY',\n",
       " 'APOLOGY',\n",
       " 'APPLICATION',\n",
       " 'APPROACHING',\n",
       " 'ARCTIC',\n",
       " 'ARE',\n",
       " 'AROUND',\n",
       " 'AS',\n",
       " 'ASCENDING',\n",
       " 'ASIA',\n",
       " 'ASIDE',\n",
       " 'ASPECT',\n",
       " 'AT',\n",
       " 'ATTACK',\n",
       " 'ATTACKED',\n",
       " 'ATTITUDES',\n",
       " 'AUGUST',\n",
       " 'AUTHOR',\n",
       " 'AZORE',\n",
       " 'Abashed',\n",
       " 'Abednego',\n",
       " 'Abel',\n",
       " 'Abjectus',\n",
       " 'Aboard',\n",
       " 'Abominable',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'Abraham',\n",
       " 'Academy',\n",
       " 'Accessory',\n",
       " 'According',\n",
       " 'Accordingly',\n",
       " 'Accursed',\n",
       " 'Achilles',\n",
       " 'Actium',\n",
       " 'Acushnet',\n",
       " 'Adam',\n",
       " 'Adieu',\n",
       " 'Adios',\n",
       " 'Admiral',\n",
       " 'Admirals',\n",
       " 'Advance',\n",
       " 'Advancement',\n",
       " 'Adventures',\n",
       " 'Adverse',\n",
       " 'Advocate',\n",
       " 'Affected',\n",
       " 'Affidavit',\n",
       " 'Affrighted',\n",
       " 'Afric',\n",
       " 'Africa',\n",
       " 'African',\n",
       " 'Africans',\n",
       " 'Aft',\n",
       " 'After',\n",
       " 'Afterwards',\n",
       " 'Again',\n",
       " 'Against',\n",
       " 'Agassiz',\n",
       " 'Ages',\n",
       " 'Ah',\n",
       " 'Ahab',\n",
       " 'Ahabs',\n",
       " 'Ahasuerus',\n",
       " 'Ahaz',\n",
       " 'Ahoy',\n",
       " 'Ain',\n",
       " 'Air',\n",
       " 'Akin',\n",
       " 'Alabama',\n",
       " 'Aladdin',\n",
       " 'Alarmed',\n",
       " 'Alas',\n",
       " 'Albatross',\n",
       " 'Albemarle',\n",
       " 'Albert',\n",
       " 'Albicore',\n",
       " 'Albino',\n",
       " 'Aldrovandi',\n",
       " 'Aldrovandus',\n",
       " 'Alexander',\n",
       " 'Alexanders',\n",
       " 'Alfred',\n",
       " 'Algerine',\n",
       " 'Algiers',\n",
       " 'Alike',\n",
       " 'Alive',\n",
       " 'All',\n",
       " 'Alleghanian',\n",
       " 'Alleghanies',\n",
       " 'Alley',\n",
       " 'Almanack',\n",
       " 'Almighty',\n",
       " 'Almost',\n",
       " 'Aloft',\n",
       " 'Alone',\n",
       " 'Alps',\n",
       " 'Already',\n",
       " 'Also',\n",
       " 'Am',\n",
       " 'Ambergriese',\n",
       " 'Ambergris',\n",
       " 'Amelia',\n",
       " 'America',\n",
       " 'American',\n",
       " 'Americans',\n",
       " 'Americas',\n",
       " 'Amittai',\n",
       " 'Among',\n",
       " 'Amsterdam',\n",
       " 'An',\n",
       " 'Anacharsis',\n",
       " 'Anak',\n",
       " 'Anatomist',\n",
       " 'And',\n",
       " 'Andes',\n",
       " 'Andrew',\n",
       " 'Andromeda',\n",
       " 'Angel',\n",
       " 'Angelo',\n",
       " 'Angels',\n",
       " 'Animated',\n",
       " 'Annawon',\n",
       " 'Anne',\n",
       " 'Anno',\n",
       " 'Anomalous',\n",
       " 'Another',\n",
       " 'Answer',\n",
       " 'Antarctic',\n",
       " 'Antilles',\n",
       " 'Antiochus',\n",
       " 'Antony',\n",
       " 'Antwerp',\n",
       " 'Anvil',\n",
       " 'Any',\n",
       " 'Anyhow',\n",
       " 'Anything',\n",
       " 'Anyway',\n",
       " 'Apollo',\n",
       " 'Apoplexy',\n",
       " 'Applied',\n",
       " 'Apply',\n",
       " 'April',\n",
       " 'Aquarius',\n",
       " 'Arch',\n",
       " 'Archbishop',\n",
       " 'Arched',\n",
       " 'Archer',\n",
       " 'Archipelagoes',\n",
       " 'Archy',\n",
       " 'Arctic',\n",
       " 'Are',\n",
       " 'Arethusa',\n",
       " 'Argo',\n",
       " 'Aries',\n",
       " 'Arion',\n",
       " 'Aristotle',\n",
       " 'Ark',\n",
       " 'Arkansas',\n",
       " 'Arkite',\n",
       " 'Arm',\n",
       " 'Armada',\n",
       " 'Arnold',\n",
       " 'Aroostook',\n",
       " 'Around',\n",
       " 'Arrayed',\n",
       " 'Arrived',\n",
       " 'Arsacidean',\n",
       " 'Arsacides',\n",
       " 'Art',\n",
       " 'Artedi',\n",
       " 'Arter',\n",
       " 'Articles',\n",
       " 'As',\n",
       " 'Asa',\n",
       " 'Ashantee',\n",
       " 'Ashore',\n",
       " 'Asia',\n",
       " 'Asiatic',\n",
       " 'Asiatics',\n",
       " 'Aside',\n",
       " 'Asphaltites',\n",
       " 'Assaulted',\n",
       " 'Assume',\n",
       " 'Assuming',\n",
       " 'Assuredly',\n",
       " 'Assyrian',\n",
       " 'Astern',\n",
       " 'Astir',\n",
       " 'Astronomy',\n",
       " 'At',\n",
       " 'Atlantic',\n",
       " 'Atlantics',\n",
       " 'Attached',\n",
       " 'Attend',\n",
       " 'August',\n",
       " 'Aunt',\n",
       " 'Australia',\n",
       " 'Australian',\n",
       " 'Austrian',\n",
       " 'Author',\n",
       " 'Authors',\n",
       " 'Auto',\n",
       " 'Availing',\n",
       " 'Avast',\n",
       " 'Avatar',\n",
       " 'Aware',\n",
       " 'Away',\n",
       " 'Awful',\n",
       " 'Ay',\n",
       " 'Aye',\n",
       " 'Azores',\n",
       " 'BACK',\n",
       " 'BACKED',\n",
       " 'BACON',\n",
       " 'BALEINE',\n",
       " 'BALLENA',\n",
       " 'BANKS',\n",
       " 'BARON',\n",
       " 'BATTLE',\n",
       " 'BE',\n",
       " 'BEALE',\n",
       " 'BEFORE',\n",
       " 'BEING',\n",
       " 'BELFAST',\n",
       " 'BELOW',\n",
       " 'BENCH',\n",
       " 'BENNETT',\n",
       " 'BERMUDAS',\n",
       " 'BETWEEN',\n",
       " 'BEWARE',\n",
       " 'BIOGRAPHY',\n",
       " 'BIT',\n",
       " 'BLACK',\n",
       " 'BLACKSMITH',\n",
       " 'BLACKSTONE',\n",
       " 'BLOOD',\n",
       " 'BLOODY',\n",
       " 'BLOWS',\n",
       " 'BOARD',\n",
       " 'BOAT',\n",
       " 'BOATS',\n",
       " 'BOOK',\n",
       " 'BOOKS',\n",
       " 'BOSOM',\n",
       " 'BOTTOM',\n",
       " 'BOUTON',\n",
       " 'BRACE',\n",
       " 'BRACTON',\n",
       " 'BREACH',\n",
       " 'BREAKERS',\n",
       " 'BREAKWATER',\n",
       " 'BRIT',\n",
       " 'BROKE',\n",
       " 'BROTHER',\n",
       " 'BROWN',\n",
       " 'BROWNE',\n",
       " 'BURKE',\n",
       " 'BURST',\n",
       " 'BUSILY',\n",
       " 'BY',\n",
       " 'Babel',\n",
       " 'Babylon',\n",
       " 'Babylonian',\n",
       " 'Bachelor',\n",
       " 'Back',\n",
       " 'Backs',\n",
       " 'Bad',\n",
       " 'Baden',\n",
       " 'Bag',\n",
       " 'Balaene',\n",
       " 'Baliene',\n",
       " 'Baling',\n",
       " 'Bally',\n",
       " 'Baltic',\n",
       " 'Baltimore',\n",
       " 'Bamboo',\n",
       " 'Bang',\n",
       " 'Banks',\n",
       " 'Barbary',\n",
       " 'Bare',\n",
       " 'Bargain',\n",
       " 'Baron',\n",
       " 'Barrens',\n",
       " 'Bartholomew',\n",
       " 'Base',\n",
       " 'Bashaw',\n",
       " 'Bashee',\n",
       " 'Basilosaurus',\n",
       " 'Bastille',\n",
       " 'Battering',\n",
       " 'Battery',\n",
       " 'Bay',\n",
       " 'Bays',\n",
       " 'Be',\n",
       " 'Beach',\n",
       " 'Beale',\n",
       " 'Beams',\n",
       " 'Bear',\n",
       " 'Bears',\n",
       " 'Beat',\n",
       " 'Because',\n",
       " 'Becket',\n",
       " 'Bedford',\n",
       " 'Beelzebub',\n",
       " 'Befooled',\n",
       " 'Before',\n",
       " 'Begone',\n",
       " 'Behold',\n",
       " 'Behring',\n",
       " 'Being',\n",
       " 'Belated',\n",
       " 'Belial',\n",
       " 'Believe',\n",
       " 'Belisarius',\n",
       " 'Bell',\n",
       " 'Bellies',\n",
       " 'Beloved',\n",
       " 'Below',\n",
       " 'Belshazzar',\n",
       " 'Belubed',\n",
       " 'Bench',\n",
       " 'Bendigoes',\n",
       " 'Beneath',\n",
       " 'Bengal',\n",
       " 'Benjamin',\n",
       " 'Bennett',\n",
       " 'Bentham',\n",
       " 'Berkshire',\n",
       " 'Berlin',\n",
       " 'Bernard',\n",
       " 'Besides',\n",
       " 'Bess',\n",
       " 'Best',\n",
       " 'Bestow',\n",
       " 'Bethink',\n",
       " 'Better',\n",
       " 'Betty',\n",
       " 'Between',\n",
       " 'Beware',\n",
       " 'Beyond',\n",
       " 'Bible',\n",
       " 'Bibles',\n",
       " 'Bibliographical',\n",
       " 'Bildad',\n",
       " 'Biographical',\n",
       " 'Birmah',\n",
       " 'Bishop',\n",
       " 'Bite',\n",
       " 'Black',\n",
       " 'Blacksmith',\n",
       " 'Blackstone',\n",
       " 'Blanc',\n",
       " 'Blanche',\n",
       " 'Blanco',\n",
       " 'Blang',\n",
       " 'Blanket',\n",
       " 'Blast',\n",
       " 'Bless',\n",
       " 'Blind',\n",
       " 'Blinding',\n",
       " 'Blocksburg',\n",
       " 'Blood',\n",
       " 'Bloody',\n",
       " 'Blue',\n",
       " 'Boat',\n",
       " 'Boats',\n",
       " 'Bobbing',\n",
       " 'Bolivia',\n",
       " 'Bombay',\n",
       " 'Bonapartes',\n",
       " 'Bone',\n",
       " 'Bones',\n",
       " 'Bonneterre',\n",
       " 'Booble',\n",
       " 'Book',\n",
       " 'Boomer',\n",
       " 'Boone',\n",
       " 'Bordeaux',\n",
       " 'Borean',\n",
       " 'Born',\n",
       " 'Borneo',\n",
       " 'Bosom',\n",
       " 'Boston',\n",
       " 'Both',\n",
       " 'Bottle',\n",
       " 'Bottom',\n",
       " 'Bourbons',\n",
       " 'Bout',\n",
       " 'Bouton',\n",
       " 'Bowditch',\n",
       " 'Bower',\n",
       " 'Boy',\n",
       " 'Boys',\n",
       " 'Brace',\n",
       " 'Brahma',\n",
       " 'Brahmins',\n",
       " 'Brandreth',\n",
       " 'Brazil',\n",
       " 'Breakfast',\n",
       " 'Bremen',\n",
       " 'Bress',\n",
       " 'Bridge',\n",
       " 'Brighggians',\n",
       " 'Bright',\n",
       " 'Bring',\n",
       " 'Brisson',\n",
       " 'Brit',\n",
       " 'Britain',\n",
       " 'British',\n",
       " 'Britons',\n",
       " 'Broad',\n",
       " 'Broadway',\n",
       " 'Broke',\n",
       " 'Brother',\n",
       " 'Browne',\n",
       " 'Brute',\n",
       " 'Buckets',\n",
       " 'Bud',\n",
       " 'Buffalo',\n",
       " 'Bulkington',\n",
       " 'Bull',\n",
       " 'Bulwarks',\n",
       " 'Bunger',\n",
       " 'Bungle',\n",
       " 'Bunyan',\n",
       " 'Buoy',\n",
       " 'Buoyed',\n",
       " 'Burke',\n",
       " 'Burkes',\n",
       " 'Burst',\n",
       " 'Burton',\n",
       " 'Burtons',\n",
       " 'Business',\n",
       " 'But',\n",
       " 'Butchers',\n",
       " 'Butler',\n",
       " 'By',\n",
       " 'Byward',\n",
       " 'C',\n",
       " 'CABIN',\n",
       " 'CABINET',\n",
       " 'CANNY',\n",
       " 'CAP',\n",
       " 'CAPTAIN',\n",
       " 'CAPTAINS',\n",
       " 'CAPTORS',\n",
       " 'CARPENTER',\n",
       " 'CATCHES',\n",
       " 'CAULKING',\n",
       " 'CEASE',\n",
       " 'CETI',\n",
       " 'CETUS',\n",
       " 'CHACE',\n",
       " 'CHAPTER',\n",
       " 'CHAPTERS',\n",
       " 'CHARLES',\n",
       " 'CHEERLY',\n",
       " 'CHEEVER',\n",
       " 'CHIEF',\n",
       " 'CHINA',\n",
       " 'CHOP',\n",
       " 'CHORUS',\n",
       " 'CHRONICLER',\n",
       " 'CIRCUMNAVIGATION',\n",
       " 'CLEAN',\n",
       " 'CLOSES',\n",
       " 'CLUSTERS',\n",
       " 'COFFIN',\n",
       " 'COILS',\n",
       " 'COLEMAN',\n",
       " 'COLL',\n",
       " 'COLNETT',\n",
       " 'COMES',\n",
       " 'COMMERCIAL',\n",
       " 'COMMODORE',\n",
       " 'COMSTOCK',\n",
       " 'CONTESTED',\n",
       " 'CONTINUES',\n",
       " 'CONVERSATIONS',\n",
       " 'COOK',\n",
       " 'COOPER',\n",
       " 'COWLEY',\n",
       " 'COWPER',\n",
       " 'CREWS',\n",
       " 'CROW',\n",
       " 'CRUISE',\n",
       " 'CRUISING',\n",
       " 'CRUIZE',\n",
       " 'CURRENTS',\n",
       " 'CUVIER',\n",
       " 'Cabaco',\n",
       " 'Cabin',\n",
       " 'Cachalot',\n",
       " 'Cadiz',\n",
       " 'Caesar',\n",
       " 'Caesarian',\n",
       " 'Cain',\n",
       " 'Calais',\n",
       " 'Californian',\n",
       " 'Call',\n",
       " 'Callao',\n",
       " 'Cambyses',\n",
       " 'Camel',\n",
       " 'Campagna',\n",
       " 'Can',\n",
       " 'Canaan',\n",
       " 'Canada',\n",
       " 'Canadian',\n",
       " 'Canal',\n",
       " 'Canaller',\n",
       " 'Canallers',\n",
       " 'Canals',\n",
       " 'Canaris',\n",
       " 'Cancer',\n",
       " 'Candles',\n",
       " 'Cannibal',\n",
       " 'Cannibals',\n",
       " 'Cannon',\n",
       " 'Canst',\n",
       " 'Cant',\n",
       " 'Canterbury',\n",
       " 'Cap',\n",
       " 'Cape',\n",
       " 'Capes',\n",
       " 'Capricornus',\n",
       " 'Captain',\n",
       " 'Captains',\n",
       " 'Capting',\n",
       " 'Caramba',\n",
       " 'Careful',\n",
       " 'Carefully',\n",
       " 'Carey',\n",
       " 'Carpenter',\n",
       " 'Carpet',\n",
       " 'Carrol',\n",
       " 'Carson',\n",
       " 'Carthage',\n",
       " 'Caryatid',\n",
       " 'Case',\n",
       " 'Cash',\n",
       " 'Cassock',\n",
       " 'Castaway',\n",
       " 'Castle',\n",
       " 'Categut',\n",
       " 'Cathedral',\n",
       " 'Catholic',\n",
       " 'Cato',\n",
       " 'Catskill',\n",
       " 'Cattegat',\n",
       " 'Caught',\n",
       " 'Cave',\n",
       " 'Caw',\n",
       " 'Cellini',\n",
       " 'Central',\n",
       " 'Certain',\n",
       " 'Certainly',\n",
       " 'Cervantes',\n",
       " 'Cetacea',\n",
       " 'Cetacean',\n",
       " 'Cetology',\n",
       " 'Cetus',\n",
       " 'Ceylon',\n",
       " 'Chace',\n",
       " 'Chaldee',\n",
       " 'Champagne',\n",
       " 'Champollion',\n",
       " 'Channel',\n",
       " 'Chapel',\n",
       " 'Charing',\n",
       " 'Charity',\n",
       " 'Charlemagne',\n",
       " 'Charley',\n",
       " 'Chart',\n",
       " 'Chartering',\n",
       " 'Chase',\n",
       " 'Cheever',\n",
       " 'Cherries',\n",
       " 'Chestnut',\n",
       " 'Chief',\n",
       " 'Childe',\n",
       " 'Chili',\n",
       " 'Chilian',\n",
       " 'China',\n",
       " 'Chinese',\n",
       " 'Cholo',\n",
       " 'Chowder',\n",
       " 'Christ',\n",
       " 'Christendom',\n",
       " 'Christian',\n",
       " 'Christianity',\n",
       " 'Christians',\n",
       " 'Christmas',\n",
       " 'Church',\n",
       " 'Cinque',\n",
       " 'Circassian',\n",
       " 'Circumambulate',\n",
       " 'Cistern',\n",
       " 'Civitas',\n",
       " 'Clam',\n",
       " 'Clap',\n",
       " 'Claus',\n",
       " 'Clay',\n",
       " 'Clear',\n",
       " 'Clearing',\n",
       " 'Cleopatra',\n",
       " 'Cleveland',\n",
       " 'Clifford',\n",
       " 'Clinging',\n",
       " 'Clootz',\n",
       " 'Close',\n",
       " 'Closing',\n",
       " 'Cloud',\n",
       " 'Cluny',\n",
       " 'Coast',\n",
       " 'Cock',\n",
       " 'Cockatoo',\n",
       " 'Cod',\n",
       " 'Cods',\n",
       " 'Coenties',\n",
       " 'Coffin',\n",
       " 'Coffins',\n",
       " 'Cognac',\n",
       " 'Coke',\n",
       " 'Cold',\n",
       " 'Coleman',\n",
       " 'Coleridge',\n",
       " 'College',\n",
       " 'Colnett',\n",
       " 'Cologne',\n",
       " 'Colonies',\n",
       " 'Colossus',\n",
       " 'Columbus',\n",
       " 'Come',\n",
       " 'Coming',\n",
       " 'Commanded',\n",
       " 'Commanders',\n",
       " 'Commend',\n",
       " 'Commodore',\n",
       " 'Commodores',\n",
       " 'Common',\n",
       " 'Commonly',\n",
       " 'Commons',\n",
       " 'Commonwealth',\n",
       " 'Companies',\n",
       " 'Comparing',\n",
       " 'Concerning',\n",
       " 'Congo',\n",
       " 'Congregation',\n",
       " 'Congregational',\n",
       " 'Conjuror',\n",
       " 'Connecticut',\n",
       " 'Consequently',\n",
       " 'Consider',\n",
       " 'Considering',\n",
       " 'Constable',\n",
       " 'Constantine',\n",
       " 'Constantinople',\n",
       " 'Consumptive',\n",
       " 'Continents',\n",
       " 'Contrasted',\n",
       " 'Conversation',\n",
       " 'Convulsively',\n",
       " 'Cook',\n",
       " 'Cooke',\n",
       " 'Cooks',\n",
       " 'Cooper',\n",
       " 'Coopman',\n",
       " 'Copenhagen',\n",
       " 'Coppered',\n",
       " 'Corinthians',\n",
       " 'Corkscrew',\n",
       " 'Corlaer',\n",
       " 'Corlears',\n",
       " 'Coronation',\n",
       " 'Corresponding',\n",
       " 'Corrupt',\n",
       " 'Cough',\n",
       " 'Could',\n",
       " 'Count',\n",
       " 'Counterpane',\n",
       " 'County',\n",
       " 'Court',\n",
       " 'Cousin',\n",
       " 'Cowper',\n",
       " 'Crab',\n",
       " 'Crack',\n",
       " 'Crammer',\n",
       " 'Crappo',\n",
       " 'Crappoes',\n",
       " 'Crazed',\n",
       " 'Creagh',\n",
       " 'Created',\n",
       " 'Cretan',\n",
       " 'Crete',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2769722",
   "metadata": {},
   "source": [
    "In NLTK, as all the \"books\" are already well \"tokenized\" and put it as a list book object. We can access ANY part of the book content by using the list indexing method. \n",
    "As below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce43976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aceess the First 20 tokens\n",
    "text[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36debccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aceess the MIDDLE content\n",
    "text[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b39812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aceess from the END\n",
    "text[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeeac40",
   "metadata": {},
   "source": [
    "### 10.8.4 Lexical Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f656f9",
   "metadata": {},
   "source": [
    "#### Token Usage Frequency (Lexical Diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20545b2",
   "metadata": {},
   "source": [
    "One may wonder: On the average, how many times we use a particular token (word) inside a document?\n",
    "\n",
    "We can do so by simply divide the total number of tokens by total number of token types - the token usage frequency or what we called \"Lexical Diversity\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text1)/len(set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text2)/len(set(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e58dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3)/len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a975dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text4)/len(set(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fc888",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "The above Python codes try to analyse the Token Usage Fequency for the FOUR text documents: Moby Dick, Sense and Sensibility, Book of Genesis and Inaugural Address Corpus. The usage frequency is ranging from 13.5 to 20.7, which is rather big in some sense. Why is that? What is the implication?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4cf09",
   "metadata": {},
   "source": [
    "####  Word Usage Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47334b0a",
   "metadata": {},
   "source": [
    "In terms of technical writing, we usually study the usage of some commonly used words such as \"the\" and \"of\". \n",
    "In the following example, we try to study is there is any pattern of the Word Usage Frequency of some commonly used words such as \"the\" in the FOUR books we have just investigated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count('the')/len(text1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82900396",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.count('the')/len(text2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.count('the')/len(text3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.count('the')/len(text4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2672a",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "1. Is there any pattern you can see as comparing these FOUR different types of literatures/documents?\n",
    "2. Check for other commonly used words such as \"of\", \"a\", \"I\" to check if there are any other pattern(s) exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71367d6a",
   "metadata": {},
   "source": [
    "## 10.9 Basic Statistical Tools in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987620db",
   "metadata": {},
   "source": [
    "### 10.9.1 Frequency Distribution - FreqDist()\n",
    "As mentioned, once NTLK can tokenize a string or even whole book of text document, the most fundamental usage on text analysis is the provision of some basic statistical tools and funtions. \n",
    "\n",
    "The first one is Frequency Distribution - FreqDist(). FreqDist() is a built-in method in NLTK to analyse the frequency distribution of every token types inside a text document. \n",
    "\n",
    "In the following example, we use Text4 - Inaugural Address Corpus as example on how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be955c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4 = FreqDist(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805bc04",
   "metadata": {},
   "source": [
    "#### FreqDist() as Dictionary Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6af49",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "As one can see, the FreqDist() will return Dictionary object of key-value pairs which can the frequency of ocurrence of every token type found inside the text document. The Key to store the Token Type name and the Value is the corresponding frequency of occurence inside the text. \n",
    "Since FreqDist() returns the Dictionary object, naturally we can use \"keys()\" to return the list of all Token Types, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "token4 = fd4.keys()\n",
    "token4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c56b8",
   "metadata": {},
   "source": [
    "#### Access FreqDist of Any Token Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb33ede",
   "metadata": {},
   "source": [
    "To get the frequency distribution of any Token Types, simply use the list item access method will do. \n",
    "Below shows the FD value of Token Type \"the\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d1cce",
   "metadata": {},
   "source": [
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "What is the FIVE commonly used Word Types (Token types without punctuations) in ANY text document?\n",
    "1. Spend FIVE minutes to make some suggestions. \n",
    "2. Use the FreqDist() to check for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339646c0",
   "metadata": {},
   "source": [
    "#### Frequency Distribution Plot from NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b355040",
   "metadata": {},
   "source": [
    "Together with FreqDist() method, NLTK provides a useful tool for us to study the top frequency distribution Token Types for any documment under investigation by using plot() function. \n",
    "FreqDist.plot() provides a powerful mean to plot the top XX most frequently used Token Types inside a text document. \n",
    "1. Using fd3 as example. First check out the documentation of FreqDist.plot() by using fd3.plot().\n",
    "2. Plot the Top 30 most frequently used Token Types in Book of Gensis (Non-Cumlative mode).\n",
    "3. Do the same plot in Cumlative mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4.plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8efdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4.plot(30,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd4.plot(30,cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d6019",
   "metadata": {},
   "source": [
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.5 Frequency Distribution for Different Literatures \n",
    "1. Ignore the punctuation, what is the TOP 5 most commonly used Word Types in Book of Gensis? Is that what you guess?\n",
    "2. Will it be the same for other great literature as well?\n",
    "3. Check this against 1) Moby Dick 2) Sense and Sensibility and 3) Inaugural Address Corpus to see if they have the same patterns. Why or Why NOT?\n",
    "4. In fact,the study of commmonly used Word Types is very important not only in NLP, but also in Cryptography. What you know why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdd5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workshop 4.5 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f59a5",
   "metadata": {},
   "source": [
    "### 10.9.2 Rare Words - Hapax\n",
    "\n",
    "\"Hapaxes\" are words that show up only once in a body of work, whether that's one publication or an entire language. \n",
    "\n",
    "The truth is: Ancient texts are full of hapaxes. For instance, in Shakespeare's work \"Love's Labour's Lost\" contains the hapax honorificabilitudinitatibus, which means \"able to achieve honors.\"\n",
    "\n",
    "In NLTK, it provides the method hapaxes() under FreqDist object to list out all the Word Types that only appeared once inside the text document. Most of them we almost neverly used. \n",
    "\n",
    "Try Adventures of Sherlock Holmes FreqDist(), you will see how useful this method is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "hap = fd.hapaxes()\n",
    "hap[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082064b9",
   "metadata": {},
   "source": [
    "<img src=\"./workshop.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "### Workshop 10.6 Learn Vocabulary using Hapaxes \n",
    "As said, Hapaxes are the great way for us to learn some new voacabulary.Especially if is contain more than (says) 12 characters or so. Why is that?\n",
    "In the following example, we try to new vocabulary by using hapaxes() together with Python in-line function implementation [w for w in hap1 if len(w) > 12] . \n",
    "Take a look how it works and do the following:\n",
    "1. Run the Python script and extract all the vocabulary (longer than 12 characters) from Moby Dick. \n",
    "2. Select FIVE meaningful vocabulary and find out their meanings. \n",
    "3. Check for \"Adventures of Sherlock Holmes\" and learn FIVE other new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workshop 4.6 Solutions\n",
    "\n",
    "voc12 = [w for w in hap if len(w) > 12]\n",
    "voc12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da538ddc",
   "metadata": {},
   "source": [
    "### 10.9.3 Collocations\n",
    "#### What is Collocations?\n",
    "\n",
    "A \"collocation\" is a familiar grouping of words, especially words that habitually appear together and thereby convey meaning by association. \n",
    "\n",
    "The word \"Collocation\" is came from the Latin for \"place together\" and was first used in its linguistic sense by British linguist John Rupert Firth (1890-1960), who famously observed, \"You shall know a word by the company it keeps.\" \n",
    "\n",
    "Think of collocations as words that usually go together. \n",
    "\n",
    "There are different kinds of collocations in English. Strong collocations are word pairings that are expected to come together, such as combinations with 'make' and 'do': You make a cup of tea, but you do your homework. \n",
    "\n",
    "Collocations are very common in business settings when certain nouns are routinely combined with certain verbs or adjectives. \n",
    "\n",
    "For example, draw up a contract, set a price, conduct negotiations, etc.\n",
    "\n",
    "#### Collocations in NLTK\n",
    "\n",
    "Collocations in NLTK are phrases or expressions containing multiple words, that are highly likely to co-occur. \n",
    "For example: social-media, school-holiday, machine-learning, Universal-Studios-US, etc.\n",
    "\n",
    "The following example try to generate collocations lists from Moby Dick, Sense and Sensibility, Book of Gensis and Inaugural Address Corpus. \n",
    "Let's take a look what are the collocation terms they extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7635132",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311403b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec20938",
   "metadata": {},
   "source": [
    "#### Bigrams\n",
    "<img src=\"./note.png\" width = \"\" height = \"\" alt=\"note\" align=left />\n",
    "\n",
    "A \"Bigram\" is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words, with special meaning in language. \n",
    "Bigram is an important feature in NLP and hold a vital key in the understanding of semantic meaning of NLP. \n",
    "In NLP Theory, Bigram is considered as an N-gram for n=2. \n",
    "We will come back to this topic in the coming lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc51796",
   "metadata": {},
   "source": [
    "## 10.10 Summary\n",
    "\n",
    "In this first NLP workshop, we introduce one of the most important NLP Python-based implementation tools – NLTK. As mentioned, NLTK is not only a basic and well-developed NLP application tool, it is also the first well organize Python-based NLP tool for text analysis and the development of NLP applications for teaching and training purpose. In this workshop, we have introduced the basics of NLTK, ranging from installation procedures to the usage for basic Text Processing and Text Analysis in NLP such as the lexical dispersion plot, which is very useful for basic text analysis. Besides, we have discussed the basics of Tokenization in NLP using NLTK, which provides a solid foundation for the coming NLP workshops on parsing and semantic analysis. \n",
    "In the next NLP workshop, we will explore how NLTK is used for N-Gram generation. We will also introduce the second important NLP implementation – spaCy, and how it can be also be used for Tokenization and Text Analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2f5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
